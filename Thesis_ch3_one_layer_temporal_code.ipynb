{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1UIUY6yedO3tQEVpwxnys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katduecker/dnn_osci/blob/main/Thesis_ch3_one_layer_temporal_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zPOyIrl0Clh",
        "outputId": "be229ecc-cadc-4057-ee91-b5ec29770e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dnn_osci'...\n",
            "remote: Enumerating objects: 454, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 454 (delta 18), reused 6 (delta 2), pack-reused 413\u001b[K\n",
            "Receiving objects: 100% (454/454), 36.08 MiB | 21.74 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n",
            "/content/dnn_osci/aet_pytorch/dnn_osci\n",
            "/content/dnn_osci/aet_pytorch/dnn_osci/aet_pytorch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/katduecker/dnn_osci\n",
        "%cd dnn_osci\n",
        "%cd aet_pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "import aet_dyn\n",
        "import aet_net\n",
        "# get CUDA index\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "eO0HxJY01w3R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euler_dyn(model,input_,params_,t_,alpha_params,DEVICE,inp_on):\n",
        "\n",
        "    # inputs:\n",
        "        # input_: input image\n",
        "        # params_: hyperparameters\n",
        "        # t_: time vector for integration\n",
        "        # alpha_params: alpha frequency & amplitude\n",
        "\n",
        "    # discretization & dynamics parameters\n",
        "    tau_h,tau_R,R,T,h_start,R_start = params_\n",
        "\n",
        "\n",
        "    # alpha frequency & amplitude\n",
        "    _af,_aa = alpha_params\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # initialize empty matrices\n",
        "        dt = np.diff(t_)[0]\n",
        "        dhdt = (torch.ones((model.dims[1],len(t_)+1))*h_start).to(DEVICE)\n",
        "        dRdt = (torch.ones((model.dims[1],len(t_)+1))*R_start).to(DEVICE)\n",
        "        dOdt = (torch.zeros((model.dims[2],len(t_)+1))).to(DEVICE)\n",
        "        dZdt = (torch.zeros((model.dims[1],len(t_)+1))).to(DEVICE)\n",
        "\n",
        "        # alpha inhibition\n",
        "        alpha_inh = _aa*np.sin(2*np.pi*_af*t_)+_aa\n",
        "\n",
        "        # preactivation (dot product of input and first weight matrix)\n",
        "        Z,_,_ = model.forw_conv(input_)\n",
        "\n",
        "        # create boxcar function to try different input onsets\n",
        "        boxcar = np.zeros_like(t_)\n",
        "        boxcar[inp_on:] = 1\n",
        "\n",
        "        # adjust initial adaptation term (threshold)\n",
        "        dRdt *= torch.max(Z)\n",
        "\n",
        "        # scale for adaptation\n",
        "        #r_scale = R*torch.max(Z).detach()\n",
        "\n",
        "\n",
        "        for _it,t in enumerate(t_):\n",
        "\n",
        "            # dynamic input: multiply input with boxcar\n",
        "            Z,_,_ = model.forw_conv(input_*boxcar[_it])\n",
        "\n",
        "            # pre-activation\n",
        "            dZdt[:,_it+1] = (Z + dhdt[:,_it] - dRdt[:,_it] - alpha_inh[_it])/T\n",
        "\n",
        "            # dynamics hidden layer\n",
        "            dhdt[:,_it+1] = dhdt[:,_it] + dt/tau_h * (-dhdt[:,_it] + model.acti1(dZdt[:,_it+1],model.sig_param))\n",
        "\n",
        "            # adaptation term\n",
        "            dRdt[:,_it+1] = dRdt[:,_it] + dt/tau_R * (-dRdt[:,_it] + R*np.mean(dZdt[:,_it+1])*dhdt[:,_it+1])\n",
        "\n",
        "            # output layer\n",
        "            dOdt[:,_it+1] = model.actiout(model.fc1(dhdt[:,_it+1]))\n",
        "\n",
        "    return dZdt, dhdt, dRdt, dOdt\n"
      ],
      "metadata": {
        "id": "BlnqsOV-2_W4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_dim_ = [28,68,3]   # [quadrant size, number of hidden nodes, number of output nodes]\n",
        "eta_ = 0.1           # learning rate\n",
        "mini_sz_ = 1          # mini batch size (1 = use SGD)\n",
        "num_epo_ = 80\n",
        "\n",
        "beta_ = 0\n",
        "p_ = 0\n",
        "kl_reg_ = [beta_,p_]#[0,0.001] # sparsity constraint parameters (not used for manual model)\n",
        "sig_param = [2, -2.5] # sigmoid slope and shift in x direction\n",
        "\n",
        "# loss function & final layer activation (for binary crossentropy use sigmoid)\n",
        "lossfun = [nn.MSELoss(), nn.Softmax(dim=-1)]\n",
        "\n",
        "params = nn_dim_,eta_,mini_sz_,num_epo_,kl_reg_,sig_param\n",
        "\n",
        "# initialize model and weights\n",
        "model = aet_net.net(params,lossfun)\n",
        "model = aet_net.init_params(model,weight_init='uni')\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=eta_)\n",
        "\n",
        "model.to(DEVICE)\n",
        "loss_hist = model.train(optimizer,noise=False,print_loss=False)\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "plt.plot(np.arange(model.num_ep),loss_hist.cpu().detach().numpy())\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "ELfbdx0V3iKR",
        "outputId": "49fcf709-010d-4d5d-9535-63857e4b0260"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-94d70fbda6c4>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dnn_osci/aet_pytorch/aet_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, dataset, noise, print_loss)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# add sparsity penalty to bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1.bias'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_regu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mget_parameter\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             raise AttributeError(\"`\" + param_name + \"` is not an \"\n\u001b[0m\u001b[1;32m    719\u001b[0m                                  \"nn.Parameter\")\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `bias` is not an nn.Parameter"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model.reg[0]:\n",
        "  print('h')"
      ],
      "metadata": {
        "id": "i_4SOV4g4OTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}